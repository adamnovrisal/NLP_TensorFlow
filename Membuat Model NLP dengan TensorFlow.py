# -*- coding: utf-8 -*-
"""Submission_NLP_MachineLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15qeOLAJbCEzg-yUscFWSquch5hBA7gAD
"""

import pandas as pd
import numpy as np
from textblob import TextBlob
import nltk, os, re, string
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from cleantext import clean
from sklearn.model_selection import train_test_split

# from tensorflow.keras.layers import Input, LSTM, Bidirectional, SpatialDropout1D, Dropout, Flatten, Dense, Embedding, BatchNormalization
# from tensorflow.keras.models import Model
# from tensorflow.keras.callbacks import EarlyStopping

# load Dataset
df = pd.read_csv('mr.csv', header = 0, encoding='utf8')
df.tail()

df = df.drop(['user','fullname','id','timestamp'], axis = 1)

df['text'] = df['text'].astype('str')

def analyze_sentiment(text):
    analysis = TextBlob(text)
    if analysis.sentiment.polarity > 0:
        return 'Positif'
    elif analysis.sentiment.polarity < 0:
        return 'Negatif'
    else:
        return 'Netral'

df["sentiment"] = df["text"].apply(lambda x : analyze_sentiment(x))
df.tail()

"""# Cleansing Data"""

def data(text):
    text = clean(text,
                 no_punct=True,
                 lower=True,
                 no_emoji=True,
                 normalize_whitespace=True
                )
    
    return text

# Remove mentions
regex_mentions = r"@[A-Za-z0-9_]+"
# Remove links
regex_links = r"https?://[A-Za-z0-9./]+"
# Remove some special characters
regex_special = r"[^A-Za-z0-9]+"
# Remove numbers 
regex_numbers = r"[0-9]+"
# Remove ordinals 
regex_ordinals = r"[0-9]+(?:st| st|nd| nd|rd| rd|th| th)"

# Remove mentions
df.text = df.text.apply(lambda x: re.sub(regex_mentions, " ", str(x).strip()))
# Remove links 
df.text = df.text.apply(lambda x: re.sub(regex_links, " ", str(x).strip()))
# Remove special characters
df.text = df.text.apply(lambda x: re.sub(regex_special, " ", str(x).strip()))
# Remove ordinals
df.text = df.text.apply(lambda x: re.sub(regex_ordinals, " ", str(x).strip()))
# Remove numbers 
df.text = df.text.apply(lambda x: re.sub(regex_numbers, " ", str(x).strip()))
# Clean tweets
df.text = df.text.apply(lambda x: data(x))

# data category one-hot-encoding
sentiment = pd.get_dummies(df.sentiment)
new = pd.concat([df, sentiment], axis=1)
new = new.drop(columns='sentiment')
new.tail()

# change dataframe value to numpy array
X = new['text'].values
y = new[['Negatif', 'Netral', 'Positif']].values

# Split data into training and validation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

# tokenizer
tokenizer = Tokenizer(num_words=5000, oov_token='x', filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')
tokenizer.fit_on_texts(X_train) 
tokenizer.fit_on_texts(X_test)
 
S_train = tokenizer.texts_to_sequences(X_train)
S_test = tokenizer.texts_to_sequences(X_test)
 
P_train = pad_sequences(S_train) 
P_test = pad_sequences(S_test)

# model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(optimizer='adam',
              metrics=['accuracy'],
              loss='categorical_crossentropy',)
model.summary()

# callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.9200 and logs.get('val_accuracy') > 0.9200):
      self.model.stop_training = True
      print("\nHas Been Reached > 92% Accuracy!!!")
callbacks = myCallback()

# model fit
Hist = model.fit(P_train, y_train, epochs=50, 
                    validation_data=(P_test, y_test), verbose=2, callbacks=[callbacks], validation_steps=30)

# plot of accuracy
import matplotlib.pyplot as plt
plt.plot(Hist.history['accuracy'])
plt.plot(Hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# plot of loss
plt.plot(Hist.history['loss'])
plt.plot(Hist.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()